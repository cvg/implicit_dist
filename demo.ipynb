{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from utils.read_matches import *\n",
    "from utils.misc import *\n",
    "import pyimplicitdist\n",
    "import poselib # https://github.com/vlarsson/PoseLib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read queries\n",
    "queries = read_matches_list('data/')\n",
    "\n",
    "images = [\n",
    "    plt.imread('data/' + q['name'])\n",
    "    for q in queries\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples\n",
    "The data folder contains 5 example 2D-3D queries from a fisheye camera.  You can change `query_idx` below to view different examples (0 to 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "query_idx = 4\n",
    "query = queries[query_idx]\n",
    "im = images[query_idx]\n",
    "\n",
    "im_height, im_width, _ = im.shape\n",
    "\n",
    "# Initialize principal point to center\n",
    "initial_pp = [im_width / 2, im_height / 2]    \n",
    "\n",
    "# Get 2D-3D matches and center keypoints\n",
    "p2d, p3d = query['matches']\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.add_subplot(2,1,1)\n",
    "plt.imshow(im)\n",
    "plt.plot(p2d[:,0], p2d[:,1], 'r.', markersize=1)\n",
    "ax = fig.add_subplot(2,1,2,projection='3d')\n",
    "ax.plot3D(p3d[:,0], p3d[:,1], p3d[:,2],'.', markersize=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Radial camera estimation\n",
    "\n",
    "First we estimate a 1D radial camera, giving us the orientation and first two components of the translation vector. Below we use poselib to do the estimation, followed by non-linear refinement of the principal point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 6.0 # px\n",
    "            \n",
    "# Initial estimation of radial camera using poselib\n",
    "p2d_center = [x - initial_pp for x in p2d]\n",
    "poselib_pose, info = poselib.estimate_1D_radial_absolute_pose(p2d_center, p3d, {\"max_reproj_error\":  threshold})\n",
    "\n",
    "# Get inlier correspondences (to the 1D radial camera)\n",
    "p2d_inlier = p2d[info[\"inliers\"]]\n",
    "p3d_inlier = p3d[info[\"inliers\"]]\n",
    "\n",
    "\n",
    "# Refine principal point\n",
    "initial_pose = pyimplicitdist.CameraPose()\n",
    "initial_pose.q_vec = poselib_pose.q\n",
    "initial_pose.t = poselib_pose.t\n",
    "out = pyimplicitdist.pose_refinement_1D_radial(p2d_inlier, p3d_inlier, initial_pose, initial_pp, pyimplicitdist.PoseRefinement1DRadialOptions())\n",
    "refined_initial_pose, pp = out['pose'], out['pp']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the radial reprojection errors\n",
    "Below we visualize the radial reprojection before, both before and after principal point refinement. Refining the principal point improves the estimate slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ind = random.sample(range(len(p2d_inlier)), 20)\n",
    "plt.imshow(im)\n",
    "plt.plot(p2d_inlier[ind,0], p2d_inlier[ind,1], 'bo')\n",
    "plot_1d_radial_reprojs(qvec2rotmat(initial_pose.q_vec), initial_pose.t, initial_pp, p3d_inlier[ind], im_width, im_height,color='r')\n",
    "plot_1d_radial_reprojs(qvec2rotmat(refined_initial_pose.q_vec), refined_initial_pose.t, pp, p3d_inlier[ind], im_width, im_height,color='g')\n",
    "plt.legend(['2D points', 'Initial 1D est.', 'Refined 1D est.'])\n",
    "\n",
    "# TODO Fix legend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the forward translations\n",
    "To perform the pose-refinement with the implicit distortion model, we first build the `CostMatrix` struct. This contains the neighbourhood information and interpolation coefficients necessary for the regularizer. \n",
    "\n",
    "Calling `pyimplicitdist.pose_refinement` optimize the full 6-DoF (including the forward translation!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_opt = pyimplicitdist.CostMatrixOptions()\n",
    "refinement_opt = pyimplicitdist.PoseRefinementOptions()\n",
    "\n",
    "cost_matrix = pyimplicitdist.build_cost_matrix(p2d_inlier, cm_opt, pp)\n",
    "pose = pyimplicitdist.pose_refinement(p2d_inlier, p3d_inlier, cost_matrix, pp, refined_initial_pose, refinement_opt)\n",
    "\n",
    "# Some simple median-based filtering to remove outliers\n",
    "out = pyimplicitdist.filter_result_pose_refinement(p2d_inlier, p3d_inlier, pose, pp, refinement_opt)\n",
    "p2d_filter, p3d_filter = np.array(out[\"points2D\"]), np.array(out[\"points3D\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the estimated pose\n",
    "\n",
    "The estimated (green) and ground-truth (red) camera pose are visualized below. Since this data is fairly easy, they overlap almost exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_q = query['qvec']\n",
    "gt_t = query['tvec']\n",
    "\n",
    "\n",
    "# For visualization we restrict ourselves to point close to the GT camera\n",
    "gt_cc = -qvec2rotmat(gt_q).T @ gt_t\n",
    "dist = [np.linalg.norm(X - gt_cc) for X in p3d_filter]\n",
    "ind = dist < 3*np.median(dist)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "#plot_camera(ax, pose.q_vec, pose.t, 10)\n",
    "ax.plot3D(p3d_filter[ind,0],p3d_filter[ind,1],p3d_filter[ind,2],'b.',markersize=1)\n",
    "plot_camera(ax, gt_q, gt_t, 1.5, 'r')\n",
    "plot_camera(ax, pose.q_vec, pose.t, 2, 'g')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recovering an explicit intrinsic calibration\n",
    "Given a camera pose and 2D-3D correspondences we can estimate an explicit (non-parametric) calibration.\n",
    "\n",
    "Here we tune the regularization strength automatically to balance the radial and tangential components of the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Estimate an explicit intrinsic calibration using the estimated pose\n",
    "\n",
    "# Re-build cost-matrix for the filtered points\n",
    "cost_matrix = pyimplicitdist.build_cost_matrix(p2d_filter, cm_opt, pp)\n",
    "calib = pyimplicitdist.calibrate(p2d_filter, p3d_filter, cost_matrix, pp, pose)\n",
    "\n",
    "print(calib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the intrinsic calibration\n",
    "The plot below shows the estimated intrinsic calibration mapping (red) against the ground-truth parametric model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_f = np.array(calib.r_f)\n",
    "theta_r = np.array(calib.theta_r)\n",
    "\n",
    "theta_min = np.min(theta_r[:,0])\n",
    "theta_max = np.max(theta_r[:,0])\n",
    "\n",
    "# Compute mapping for the GT parametric model\n",
    "dist_params = query['camera']['params']\n",
    "pp_gt = query['camera']['params'][2:4]\n",
    "thetas = np.linspace(theta_min, theta_max,20)\n",
    "r_gt = np.array([\n",
    "        np.linalg.norm(x-pp_gt)\n",
    "        for x in apply_opencv_distortion(\n",
    "            [np.array([np.sin(theta),0,np.cos(theta)]) for theta in thetas],\n",
    "        dist_params)\n",
    "        ])\n",
    "f_gt = r_gt / np.tan(thetas)\n",
    "\n",
    "# Compute the \"raw\" mapping\n",
    "R = qvec2rotmat(pose.q_vec)\n",
    "Xcam = np.array([R @ Z + pose.t for Z in p3d_filter])\n",
    "\n",
    "r_raw = [np.linalg.norm(x - pp) for x in p2d_filter]\n",
    "f_raw = [r*r * Z[2] / np.dot(x-pp, Z[0:2]) for (r,x,Z) in zip(r_raw, p2d_filter, Xcam)]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(r_gt,f_gt,'bo')\n",
    "plt.plot(r_f[:,0], r_f[:,1],'r-')\n",
    "plt.legend(['GT calib','Est. calib'])\n",
    "plt.xlabel('image radius r_i')\n",
    "plt.ylabel('pointwise focal length f_i')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(r_raw,f_raw,'.',markersize=1)\n",
    "plt.legend(['Raw f_i'])\n",
    "plt.xlabel('image radius r_i')\n",
    "plt.ylabel('pointwise focal length f_i')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprojecting into the image\n",
    "Calling the function `pyimplicitdist.distort(points3D, calib)` we can project into the image. Note that the 3D points should be in the camera coordinate system (i.e. after applying R,t)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project into the image with the estimated calibration\n",
    "\n",
    "R = qvec2rotmat(pose.q_vec)\n",
    "Xcam = np.array([R @ Z + pose.t for Z in p3d_filter])\n",
    "\n",
    "proj = np.array(pyimplicitdist.distort(Xcam, calib))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.imshow(im)\n",
    "plt.plot(proj[:,0], proj[:,1],'r.')\n",
    "plt.plot(p2d_filter[:,0], p2d_filter[:,1],'g+')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the estimated mapping to undistort the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_fov = 120 / 180 * np.pi\n",
    "subsample = 5\n",
    "target_h = int(np.ceil(im_height / subsample))\n",
    "target_w = int(np.ceil(im_width / subsample))\n",
    "\n",
    "target_focal = target_w / 2 / np.tan(target_fov/2)\n",
    "\n",
    "ii, jj = np.meshgrid(np.linspace(1,target_w,target_w), np.linspace(1,target_h,target_h), indexing='ij')\n",
    "ii = ii - 0.5\n",
    "jj = jj - 0.5\n",
    "\n",
    "# Remove principal point\n",
    "ii = ii - target_w / 2\n",
    "jj = jj - target_h / 2\n",
    "\n",
    "\n",
    "X = np.c_[ ii.flatten(), jj.flatten(), target_focal * np.ones(np.prod(ii.shape)) ]\n",
    "x = np.array(pyimplicitdist.distort(X, calib))\n",
    "im_undist = np.zeros((target_h, target_w, 3), dtype=im.dtype)\n",
    "for i in range(target_w):\n",
    "    for j in range(target_h):\n",
    "        coord = x[j + i*target_h]\n",
    "        if np.any(coord < 0) or coord[1] > im_height-1 or coord[0] > im_width-1:\n",
    "            continue          \n",
    "        if np.any(np.isnan(coord)):\n",
    "            continue\n",
    "        im_undist[j,i] = im[int(coord[1]), int(coord[0])]\n",
    "        \n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(im_undist)\n",
    "plt.subplot(2,1,2)\n",
    "plt.imshow(im)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
